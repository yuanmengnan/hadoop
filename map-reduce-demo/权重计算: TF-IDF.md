# TF-IDF
96  snoweek 
2016.02.11 14:53* 字数 579 阅读 210评论 0喜欢 0
TF-IDF（term frequency–inverse document frequency），即词频-逆向文档频率。其中词频表示一个词在一个文档中出现的频率（注意，不是次数，），逆向文档频率与文档频率是相反的概念，文档频率指语料库中包含某个词的文档数，文章数越多，文档频率越大，则逆向文档频率越小。

TF-IDF是一种统计方法，用以评估一个词语对于文档集合中的一份文档的重要程度。字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在其他文档中出现的频率成反比下降。如果词语t在一篇文档d中出现的频率高，并且在其他文档中很少出现，则认为词w具有很好的区分能力，适合用来把文档d和其他文档区分开来。

上面讲了，词频指的是频率而不是次数，因为一个词语在长文档里可能会比短文档有更高的词数，而不管该词语重要与否。
df:文档频率,与idf相反
TF-IDF=tfidf=tfdf=tf*log(n/df)
idf=log(n/df),因为很有df=0，即词语t不在文档集合中,那显然该公式是没有办法计算的，所以通常都用x+1.
某一特定文档内的高词语频率，以及该词语在整个文件当中的低文档频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。最后筛选出某个文档的关键词。

目前，TF-IDF方法广泛用于搜索引擎中，首先我们输入由关键词t[1]…t[n]组成的查询串q，他先计算每个词对于一个文档的TF-IDF然后根据每个关键词的TF-IDF计算出一个文档d和查询串q计算一个权值，用于表示查询串q与文档d的匹配度。
匹配度=sum（tf-idf[1]+...+tf-idf[n]）
